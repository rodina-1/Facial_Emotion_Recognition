# -*- coding: utf-8 -*-
"""DeepLearning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DZxBWKYUSKtlGKaCYXUmpvURvUSg2uoU
"""

import torch
import torch.nn as nn
import torch.optim as optim # to optimize nural network
from torchvision import transforms
from torchvision.datasets import ImageFolder
from torch.utils.data import  DataLoader, random_split
from PIL import Image
import cv2
import os

device= torch.device('cuda')
print(device)

import kagglehub

# Download latest version
path = kagglehub.dataset_download("shuvoalok/raf-db-dataset")

print("Path to dataset files:", path)

"""#  Data Preprocessing

Resizing â€“ Normalization â€“ Augmentation
"""

train_transforms = transforms.Compose([
    transforms.Grayscale(num_output_channels=1),   # Ù†Ø®Ù„ÙŠ Ø§Ù„ØµÙˆØ±Ø© channel=1 Ù„Ø£Ù† Ø§Ù„Ø¯Ø§ØªØ§ grayscale
    transforms.Resize((48, 48)),                   # Ù†Ø«Ø¨Øª Ø§Ù„Ø­Ø¬Ù… Ø¹Ù„Ù‰ 48x48
    transforms.RandomHorizontalFlip(),             # Augmentation
    transforms.RandomRotation(10),                 # Augmentation
    transforms.ToTensor(),                         # Ù†Ø­ÙˆÙ„ Ø§Ù„ØµÙˆØ±Ø© Ù„ØªÙ†Ø³ÙˆØ± (0 â†’ 1)
    transforms.Normalize([0.5], [0.5])             # Ù†Ø¹Ù…Ù„ normalizing
])

test_transforms = transforms.Compose([
    transforms.Grayscale(num_output_channels=1),
    transforms.Resize((48, 48)),
    transforms.ToTensor(),
    transforms.Normalize([0.5], [0.5])
])

"""Data uploading using dataset.ImageFolder"""

path = path +"/DATASET"

print(path)

from torchvision.datasets import ImageFolder

train_dataset = ImageFolder(root=os.path.join(path, "train"), transform=train_transforms)
test_dataset = ImageFolder(root=os.path.join(path, "test"), transform=test_transforms)

print("Train size:", len(train_dataset))
print("Test size:", len(test_dataset))

"""DataLoader"""

train_loader = DataLoader(
    train_dataset,
    batch_size=64,
    shuffle=True,
    num_workers=2
)

test_loader = DataLoader(
    test_dataset,
    batch_size=64,
    shuffle=False,
    num_workers=2
)

expression_map = {
    '1': 'Surprise',
    '2': 'Fear',
    '3': 'Disgust',
    '4': 'Happy',
    '5': 'Sad',
    '6': 'Angry',
    '7': 'Neutral'
}

import matplotlib.pyplot as plt
import math

class_names = train_dataset.classes
print(class_names)

def show_batch_images(dataloader, class_names, num_images):
    images, labels = next(iter(dataloader))

    plt.figure(figsize=(12, 10))

    # Calculate rows and columns for a nearly square grid
    n_cols = math.ceil(math.sqrt(num_images))
    n_rows = math.ceil(num_images / n_cols)

    for i in range(num_images):
        img = images[i].numpy().squeeze()

        img = (img * 0.5) + 0.5

        plt.subplot(n_rows, n_cols, i + 1)
        plt.imshow(img, cmap="gray")
        plt.title(f"{labels[i].item()} â†’ {expression_map[class_names[labels[i].item()]]}")
        plt.axis("off")

    plt.tight_layout()
    plt.show()

# Call the function with an integer representing the number of images to display
show_batch_images(train_loader, class_names, num_images=len(expression_map))

from collections import Counter
import matplotlib.pyplot as plt

def plot_class_distribution(dataset, title="Class Distribution"):
    labels = dataset.targets
    # Get the list of string class names from the dataset (e.g., ['1', '2', ...])
    # These correspond to the integer labels (0, 1, ...)
    dataset_class_names = dataset.classes
    counts = Counter(labels)

    plt.figure(figsize=(10,5))
    # Map integer labels to string class names, then to emotion names using expression_map
    plt.bar([expression_map[dataset_class_names[i]] for i in counts.keys()], counts.values())
    plt.title(title)
    plt.ylabel("Number of Images")
    plt.xticks(rotation=45)
    plt.show()

    # Most Expression
    max_label, max_count = counts.most_common(1)[0]
    # Use dataset_class_names to get the string label, then expression_map for the emotion name
    print(f"Most Expression: {expression_map[dataset_class_names[max_label]]} ({max_count} images)")


plot_class_distribution(train_dataset, "Train Dataset Distribution")
plot_class_distribution(test_dataset, "Test Dataset Distribution")

"""# CNN Model Design"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
import numpy as np
from sklearn.utils import class_weight
import os

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
INPUT_SHAPE = (48, 48)
NUM_CLASSES = 7

class FER_CNN(nn.Module):
    def __init__(self):
        super(FER_CNN, self).__init__()

        self.features = nn.Sequential(
            # Block 1
            nn.Conv2d(1, 64, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.BatchNorm2d(64),
            nn.MaxPool2d(2),
            nn.Dropout(0.25),

            # Block 2
            nn.Conv2d(64, 128, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.BatchNorm2d(128),
            nn.MaxPool2d(2),
            nn.Dropout(0.25),

            # Block 3
            nn.Conv2d(128, 256, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.BatchNorm2d(256),
            nn.MaxPool2d(2),
            nn.Dropout(0.30),

            # Block 4
            nn.Conv2d(256, 512, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.BatchNorm2d(512),
            nn.MaxPool2d(2),
            nn.Dropout(0.40),
        )

        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Linear(512 * 3 * 3, 256),   # 48â†’24â†’12â†’6â†’3
            nn.ReLU(),
            nn.Dropout(0.40),
            nn.Linear(256, NUM_CLASSES)
        )

    def forward(self, x):
        x = self.features(x)
        x = self.classifier(x)
        return x

def build_fer_cnn():
    return FER_CNN().to(DEVICE)

model = build_fer_cnn()
print(model)

train_path = "/kaggle/input/fer2013/train"
val_path   = "/kaggle/input/fer2013/test"

train_transform = transforms.Compose([
    transforms.Grayscale(),
    transforms.Resize(INPUT_SHAPE),
    transforms.RandomRotation(10),
    transforms.RandomHorizontalFlip(),
    transforms.RandomAffine(0, translate=(0.1, 0.1)),
    transforms.ToTensor()
])

val_transform = transforms.Compose([
    transforms.Grayscale(),
    transforms.Resize(INPUT_SHAPE),
    transforms.ToTensor()
])

train_dataset = datasets.ImageFolder(train_path, transform=train_transform)
val_dataset   = datasets.ImageFolder(val_path,   transform=val_transform)

train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
val_loader   = DataLoader(val_dataset,   batch_size=64, shuffle=False)

y_train = np.array(train_dataset.targets)
classes_present = np.unique(y_train)

weights = class_weight.compute_class_weight(
    class_weight='balanced',
    classes=classes_present,
    y=y_train
)

class_weights = torch.tensor(weights, dtype=torch.float).to(DEVICE)

criterion = nn.CrossEntropyLoss(weight=class_weights)
optimizer = optim.Adam(model.parameters(), lr=1e-3)

scheduler = optim.lr_scheduler.ReduceLROnPlateau(
    optimizer, factor=0.5, patience=5
)

# # ===============================
# #  Hyperparameters
# # ===============================
# MAX_EPOCHS = 100
# PATIENCE = 10

# best_val_acc = 0
# early_stop_counter = 0

# history = {"loss": [], "val_loss": [], "accuracy": [], "val_accuracy": []}

# # ===============================
# #  Training Loop
# # ===============================
# for epoch in range(MAX_EPOCHS):
#     # ---- Training ----
#     model.train()
#     running_loss = 0
#     correct = 0
#     total = 0

#     for images, labels in train_loader:
#         images, labels = images.to(DEVICE), labels.to(DEVICE)

#         optimizer.zero_grad()
#         outputs = model(images)
#         loss = criterion(outputs, labels)
#         loss.backward()
#         optimizer.step()

#         running_loss += loss.item()
#         _, predicted = torch.max(outputs, 1)
#         correct += (predicted == labels).sum().item()
#         total += labels.size(0)

#     train_loss = running_loss / len(train_loader)
#     train_acc = correct / total

#     # ---- Validation ----
#     model.eval()
#     val_loss = 0
#     val_correct = 0
#     val_total = 0

#     with torch.no_grad():
#         for images, labels in val_loader:
#             images, labels = images.to(DEVICE), labels.to(DEVICE)

#             outputs = model(images)
#             loss = criterion(outputs, labels)
#             val_loss += loss.item()

#             _, predicted = torch.max(outputs, 1)
#             val_correct += (predicted == labels).sum().item()
#             val_total += labels.size(0)

#     val_loss = val_loss / len(val_loader)
#     val_acc = val_correct / val_total

#     # ---- Print stats ----
#     print(f"Epoch {epoch+1}/{MAX_EPOCHS} | "
#           f"train_acc={train_acc:.4f}, val_acc={val_acc:.4f}, "
#           f"train_loss={train_loss:.4f}, val_loss={val_loss:.4f}")

#     # ---- Save history ----
#     history["loss"].append(train_loss)
#     history["accuracy"].append(train_acc)
#     history["val_loss"].append(val_loss)
#     history["val_accuracy"].append(val_acc)

#     # ---- Update learning rate ----
#     scheduler.step(val_loss)

#     # ---- Early stopping + Best model checkpoint ----
#     if val_acc > best_val_acc:
#         torch.save(model.state_dict(), "best_model.pth")  # Ø­ÙØ¸ Ø£ÙØ¶Ù„ Ù…ÙˆØ¯ÙŠÙ„
#         best_val_acc = val_acc
#         early_stop_counter = 0
#         print(f"Best model updated at epoch {epoch+1} with val_acc={val_acc:.4f}")
#     else:
#         early_stop_counter += 1

#     if early_stop_counter >= PATIENCE:
#         print(f"Early stopping activated at epoch {epoch+1}")
#         break



"""# Pre-trained SWIN Transformer"""

!pip install timm

import timm
from PIL import Image
import torch.nn.functional as F
from google.colab import files

"""## preprocessing for swin"""

train_transform = transforms.Compose([
    transforms.Resize((224,224)),
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(15),
    transforms.ColorJitter(brightness=0.3, contrast=0.3),
    transforms.RandomResizedCrop(224, scale=(0.8,1.0)),
    transforms.ToTensor(),
    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])
])

test_transform = transforms.Compose([
    transforms.Resize((224,224)),
    transforms.ToTensor(),
    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])
])

path = path +"/DATASET"
print(path)
train_dataset = ImageFolder(root=os.path.join(path, "train"), transform=train_transform)
test_dataset = ImageFolder(root=os.path.join(path, "test"), transform=test_transform)

from torch.utils.data import DataLoader, WeightedRandomSampler
from collections import Counter
import torch # Ensure torch is imported for DEVICE definition

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# =========================
# Ø­Ø³Ø§Ø¨ Ø£ÙˆØ²Ø§Ù† Ø§Ù„ÙƒÙ„Ø§Ø³Ø§Øª Ù„Ù„ØªÙˆØ§Ø²Ù†
# =========================
train_labels_full = [train_dataset.targets[i] for i in range(len(train_dataset))]
counts = Counter(train_labels_full)
num_classes = len(train_dataset.classes)
weights = [1.0 / counts[i] for i in range(num_classes)]
class_weights = torch.FloatTensor(weights).to(DEVICE)
print("Class Weights:", class_weights)

# =========================
# Loss Function Ù…Ø¹ Ø§Ù„Ø£ÙˆØ²Ø§Ù†
# =========================
criterion = nn.CrossEntropyLoss(weight=class_weights)

# =========================
# Ø¥Ø¹Ø¯Ø§Ø¯ WeightedRandomSampler Ù„Ù„Ù€ DataLoader
# =========================
sample_weights = [class_weights[train_dataset.targets[i]] for i in range(len(train_dataset))]
sampler = WeightedRandomSampler(
    weights=sample_weights,
    num_samples=len(sample_weights),
    replacement=True
)

train_loader = DataLoader(
    train_dataset,
    batch_size=64,
    shuffle=True,
    num_workers=2
)

test_loader = DataLoader(
    test_dataset,
    batch_size=64,
    shuffle=False,
    num_workers=2
)

class_names = train_dataset.classes
num_classes = len(class_names)
print("Classes:", class_names)
print("Number of classes:", num_classes)

"""### Model Load"""

model = timm.create_model("swin_tiny_patch4_window7_224", pretrained=True)
model.head = nn.Linear(model.head.in_features, num_classes)  # Linear head
model = model.to(device)

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=1e-4)
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3)

"""# **Training**"""

# Training Parameters
# -------------------
best_acc = 0
EPOCHS = 60
patience = 10           # Early stopping patience
trigger_times = 0
best_val_loss = float('inf')
checkpoint_interval = 5

for epoch in range(1, EPOCHS+1):
    print(f"\nEpoch {epoch}/{EPOCHS}")

    # ---- TRAIN ----
    model.train()
    train_correct = 0
    train_loss = 0

    for imgs, labels in train_loader:
        imgs, labels = imgs.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model(imgs)
        # Apply global average pooling over spatial dimensions (H, W)
        outputs = outputs.mean(dim=[-1, -2])
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        train_loss += loss.item()
        train_correct += (outputs.argmax(1) == labels).sum().item()

    train_acc = train_correct / len(train_dataset)
    train_loss /= len(train_loader)

    # ---- VALIDATION ----
    model.eval()
    val_correct = 0
    val_loss = 0

    with torch.no_grad():
        for imgs, labels in test_loader:
            imgs, labels = imgs.to(device), labels.to(device)
            outputs = model(imgs)
            # Apply global average pooling over spatial dimensions (H, W)
            outputs = outputs.mean(dim=[-1, -2])
            loss = criterion(outputs, labels)
            val_loss += loss.item()
            val_correct += (outputs.argmax(1) == labels).sum().item()

    val_acc = val_correct / len(test_dataset)
    val_loss /= len(test_loader)
    scheduler.step(val_loss)

    print(f"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | "
          f"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}")

    # ---- Save Best Model ----
    if val_acc > best_acc:
        best_acc = val_acc
        torch.save({
            'model_state_dict': model.state_dict(),
            'class_names': class_names
        }, "best_swin_dataset.pth")
        print(" Best model saved!")

    # ---- Save Checkpoint ----
    if epoch % checkpoint_interval == 0:
        torch.save({
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'class_names': class_names,
            'best_acc': best_acc
        }, f"checkpoint_epoch_{epoch}.pth")
        print(f"Checkpoint saved at epoch {epoch}")

    # ---- Early Stopping ----
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        trigger_times = 0
    else:
        trigger_times += 1
        if trigger_times >= patience:
            print(f"\nâ¹ Early stopping triggered at epoch {epoch}")
            break

print("\nFinished Training. Best Val Acc:", best_acc)

# -------------------
# Load best model
# -------------------
checkpoint = torch.load("best_swin_dataset.pth")
model.load_state_dict(checkpoint['model_state_dict'])
class_names = checkpoint['class_names']
model.eval()

import os
print(os.path.getsize("best_swin_dataset.pth"))

"""# **Evaluation**"""

expression_map = {
    '1': 'Surprise',
    '2': 'Fear',
    '3': 'Disgust',
    '4': 'Happy',
    '5': 'Sad',
    '6': 'Angry',
    '7': 'Neutral'
}

import numpy as np
import pandas as pd
import torch
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

def full_evaluation(model, test_loader, expression_map, device):
    all_preds = []
    all_labels = []

    # Convert expression_map keys to sorted label order
    # Example: {'1': 'Surprise'} â†’ class_names = ['Surprise', ...]
    class_names = [expression_map[str(i)] for i in sorted([int(k) for k in expression_map.keys()])]

    model.eval()

    with torch.no_grad():
        for images, labels in test_loader:
            images = images.to(device)
            labels = labels.to(device)

            outputs = model(images)

            # For models that return feature maps (e.g., Swin)
            if outputs.dim() > 2:
                outputs = outputs.mean(dim=[-1, -2])

            _, preds = torch.max(outputs, 1)

            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    # =====================
    # Overall Accuracy
    # =====================
    acc = accuracy_score(all_labels, all_preds)
    print("âœ… Overall Accuracy:", acc)

    # =====================
    # Classification Report
    # =====================
    report = classification_report(
        all_labels,
        all_preds,
        target_names=class_names,
        digits=4
    )

    print("\nğŸ“Š Classification Report:\n")
    print(report)

    # =====================
    # Save Report as CSV
    # =====================
    report_dict = classification_report(
        all_labels,
        all_preds,
        target_names=class_names,
        output_dict=True,
        digits=4
    )

    df_report = pd.DataFrame(report_dict).transpose()
    df_report.to_csv("evaluation_report.csv")
    print("ğŸ“ Report saved as evaluation_report.csv")

    # =====================
    # Confusion Matrix
    # =====================
    cm = confusion_matrix(all_labels, all_preds)

    confusion_pairs = []
    for i in range(len(class_names)):
        for j in range(len(class_names)):
            if i != j and cm[i][j] > 0:
                confusion_pairs.append((class_names[i], class_names[j], cm[i][j]))

    confusion_pairs = sorted(confusion_pairs, key=lambda x: x[2], reverse=True)

    print("\nğŸ” Most Confused Class Pairs:")
    for pair in confusion_pairs[:5]:
        print(f"{pair[0]} â†’ {pair[1]} : {pair[2]} times")

    # =====================
    # Accuracy + F1 per Class
    # =====================
    class_accuracy = cm.diagonal() / cm.sum(axis=1)
    class_f1 = df_report.loc[class_names, "f1-score"].values

    print("\nğŸ“Œ Per-Class Performance:")
    for i, class_name in enumerate(class_names):
        print(f"{class_name} | Accuracy: {class_accuracy[i]:.4f} | F1-score: {class_f1[i]:.4f}")

    return {
        "overall_accuracy": acc,
        "class_accuracy": class_accuracy,
        "class_f1": class_f1,
        "confusion_matrix": cm,
        "report_dataframe": df_report
    }

results = full_evaluation(model, test_loader, expression_map, device)

import seaborn as sns
import matplotlib.pyplot as plt

# Confusion Matrix
cm = results["confusion_matrix"]
labels = [expression_map[str(i)] for i in range(1, len(expression_map)+1)]
plt.figure(figsize=(10,8))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
            xticklabels=labels,
            yticklabels=labels)
plt.xlabel("Predicted Class")
plt.ylabel("Actual Class")
plt.title("Confusion Matrix")
plt.xticks(rotation=45)
plt.yticks(rotation=0)
plt.tight_layout()
plt.show()

from google.colab import files
from PIL import Image
import torch
from torchvision import transforms
import torch.nn.functional as F

# -------------------
# Expression mapping
# -------------------
expression_map = {
    '1': 'Surprise',
    '2': 'Fear',
    '3': 'Disgust',
    '4': 'Happy',
    '5': 'Sad',
    '6': 'Angry',
    '7': 'Neutral'
}

# -------------------
# Upload image
# -------------------
uploaded = files.upload()
image_path = list(uploaded.keys())[0]

# -------------------
# Open and preprocess
# -------------------
img = Image.open(image_path).convert("RGB")
transform = transforms.Compose([
    transforms.Resize((224,224)),
    transforms.ToTensor(),
    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])
])
input_tensor = transform(img).unsqueeze(0).to(device)  # add batch dimension

# -------------------
# Inference
# -------------------
model.eval()  # ØªØ£ÙƒØ¯ Ø§Ù† Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ ÙÙŠ eval mode
with torch.no_grad():
    output = model(input_tensor)               # shape = [1, num_classes, H, W] if not pooled
    output = output.mean(dim=[-1, -2])        # Apply global average pooling
    probs = F.softmax(output, dim=1)           # probability Ù„ÙƒÙ„ class
    pred_idx = probs.argmax(1).item()          # index Ø£Ø¹Ù„Ù‰ probability
    pred_class_number = class_names[pred_idx]  # Ø§Ù„Ø§Ø³Ù… Ø§Ù„Ø£ØµÙ„ÙŠ Ù…Ù† dataset
    pred_class_name = expression_map.get(pred_class_number, pred_class_number)  # mapping Ù„Ù„ØªØ³Ù…ÙŠØ©

print(f"Predicted Class: {pred_class_name}")